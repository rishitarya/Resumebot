{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishitarya/Resumebot/blob/main/Ask_Rishit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkc4mfkPNt3G"
      },
      "source": [
        "# **Ask Rishit**\n",
        "### This is a chatbot where you can ask professional questions to Rishit\n",
        "#### *Press **Cmd/Cntrl + F9** to run this notebook, you will see the chatbot loaded below withing 4 mins (Connect to T4 runtime if not by default)*\n",
        "\n",
        "######This chatbot is based on fine tuned phi3 model and gte embeddings.\n",
        "######Please feel free to reach out to me at [Linkedin](https://www.linkedin.com/in/rishit-arya/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-djrnzgYgvWq"
      },
      "outputs": [],
      "source": [
        "#@title Loading Libraries\n",
        "%%capture\n",
        "!pip install -q gradio;\n",
        "!pip install -q langchain;\n",
        "!pip install -q langchain_community;\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "#!pip install \".[xformers]\" --upgrade --force-reinstall --extra-index-url https://download.pytorch.org/whl/cu118;\n",
        "#!pip install -q --no-deps \"xformers<0.0.26\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "from torch import __version__; from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
        "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton\n",
        "!pip install torch==2.0.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install bitsandbytes\n",
        "!pip install -q sentence_transformers;\n",
        "!pip install -q huggingface_hub;\n",
        "!pip install -q faiss-gpu;\n",
        "!pip install -q faiss;\n",
        "!pip install -q triton triton.common;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV8emdMViDGN",
        "outputId": "8d760d6b-e3eb-4602-b755-6d7843cf7a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: faiss-gpu-cu12 in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (24.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.5.8)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu-cu12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "J12tt9FIh_gd"
      },
      "outputs": [],
      "source": [
        "#@title Loading the model\n",
        "%%capture\n",
        "from unsloth import FastLanguageModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.embeddings import GooglePalmEmbeddings, HuggingFaceEmbeddings\n",
        "import faiss.contrib.torch_utils\n",
        "import faiss\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "model_name = 'aryarishit/mistral-unsloth-resumebot-lora_adaptar-v2'\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        ")\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "with open(hf_hub_download(repo_id=\"aryarishit/Info_docs\", filename=\"Info_doc.txt\",repo_type='dataset')) as f:\n",
        "    contexts = f.read()\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=256,\n",
        "    separators = ['\\n\\n','\\n','  '],\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "contexts = text_splitter.split_text(contexts)\n",
        "contexts = [x.strip() for x in contexts]\n",
        "contexts = np.array([x for x in contexts if len(x) > 0])\n",
        "\n",
        "encoder = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5',trust_remote_code=True)\n",
        "context_vectors = encoder.encode(contexts)\n",
        "print(1)\n",
        "print(context_vectors)\n",
        "index = faiss.IndexFlatL2(context_vectors.shape[1])\n",
        "faiss.normalize_L2(context_vectors)\n",
        "index.add(context_vectors)\n",
        "\n",
        "def get_context(query,top_k = 3):\n",
        "    top_k = top_k\n",
        "\n",
        "    query_vector = encoder.encode([query])\n",
        "    faiss.normalize_L2(query_vector)\n",
        "    top_k = top_k\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "    context = \"\"\n",
        "    for i in range(len(indices[0])):\n",
        "        context = context + contexts[indices[0][i]] + \"\\n\\n\"\n",
        "    return context\n",
        "\n",
        "def get_alpaca_prompt(context,query):\n",
        "    instruction_string = '''[INST]Consider you are assistant to Rishit Arya, and answers on behalf of him, Given the following context and a question, generate an answer based on the given context only. If the answer to the question is not found in the context, strictly state \"I was not provided this info\" only, don't try to make up an answer.Answer pricesly to what is asked it as if you are answering to Rishit's potential client. \\nContext:{}\n",
        "Question:{}[\\INST] \\nAnswer:'''\n",
        "\n",
        "    prompt = instruction_string.format(\n",
        "            context,\n",
        "            query # input\n",
        "        )\n",
        "    return prompt\n",
        "\n",
        "def generate_response(query,history):\n",
        "    context = get_context( query)\n",
        "    print(context)\n",
        "    prompt = get_alpaca_prompt(context,query)\n",
        "    # print(prompt)\n",
        "    inputs = tokenizer([prompt],return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 200,use_cache=True)\n",
        "    return tokenizer.batch_decode(outputs)[0].split('Answer:')[1].replace('<|endoftext|>','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHWfLwRXH-HN",
        "outputId": "866c6f26-978f-43af-eb02-d872e14d1dab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-6586eb4a12d5>:6: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(value=[(None, \"Welcome ðŸ‘‹. What would you like to know?\")],height=250),\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:317: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://38a117ec65c54acda3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://38a117ec65c54acda3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, My name is Rishit Arya. I am working as a Data Scientist at Walmart Global Tech India which is based in Bengaluru, India. I have a degree in Chamical Engineering from IIT BHU Varanasi. I have interest in Machine learning modelling and statistical analysis. I believe in power of data and the effect it can have in shaping the world, I am always looking for opportunities where I can explore and experience this power first hand.\n",
            "\n",
            "I have two personal projects here, American type option pricing through modified Black Scholes model and Resume chatbot.\n",
            "\n",
            "I love to play cricket, piano, researching about random stuffs, Here is the to my medium blogs where I try to write some of these researches as blog.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1648, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 857, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 862, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-3-85f467ea8497>\", line 70, in generate_response\n",
            "    inputs = tokenizer([prompt],return_tensors='pt').to('cuda')\n",
            "             ^^^^^^^^^\n",
            "NameError: name 'tokenizer' is not defined\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://06a9587e0b66a3e34e.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://38a117ec65c54acda3.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Loading the app\n",
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(generate_response,\n",
        "                 title='Ask Rishit',\n",
        "                 chatbot=gr.Chatbot(value=[(None, \"Welcome ðŸ‘‹. What would you like to know?\")],height=250),\n",
        "                 examples = ['Tell me about his medium articles','What are his previous experiences?','What is his educational background?']).launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7c5mT-uqjMg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMgu4Az34elXoKnF85nQCm2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}